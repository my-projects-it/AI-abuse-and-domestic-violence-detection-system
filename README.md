üíú AI-Based Abuse & Domestic Violence Detection System üõ°Ô∏è

üö∫ Empowering Women. Protecting Lives. Enabling Safety.

üéØ ‡§Ø‡§π AI-powered Streamlit Web App ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü, ‡§ë‡§°‡§ø‡§Ø‡•ã ‡§î‡§∞ ‡§á‡§Æ‡•á‡§ú ‡§°‡•á‡§ü‡§æ ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§ï‡•á Domestic Violence ‡§î‡§∞ Abuse Detection ‡§ï‡§∞‡§§‡§æ ‡§π‡•à ‚Äî instantly, intelligently ‡§î‡§∞ ‡§Ü‡§∏‡§æ‡§®‡•Ä ‡§∏‡•á!

üöÄ Features

üî† Text Detection:
‚û°Ô∏è Logistic Regression + TF-IDF ‡§ï‡•á ‡§ú‡§º‡§∞‡§ø‡§è abusive keywords ‡§∏‡•á ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§ï‡§æ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§ï‡§∞‡§§‡§æ ‡§π‡•à‡•§

üîä Audio Detection:
‚û°Ô∏è `librosa` ‡§∏‡•á MFCC ‡§´‡•Ä‡§ö‡§∞‡•ç‡§∏ ‡§è‡§ï‡•ç‡§∏‡§ü‡•ç‡§∞‡•à‡§ï‡•ç‡§ü ‡§ï‡§∞‡§ï‡•á verbally abusive ‡§Ü‡§µ‡§æ‡§ú‡§º ‡§ï‡§æ ‡§Ö‡§®‡•Å‡§Æ‡§æ‡§® ‡§≤‡§ó‡§æ‡§§‡§æ ‡§π‡•à‡•§

üß† Image (Face) Detection:
‚û°Ô∏è `DeepFace` ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•á ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ ‡§∏‡•á facial emotion ‡§ï‡•ã ‡§™‡§π‡§ö‡§æ‡§® ‡§ï‡§∞ possible physical abuse ‡§ï‡§æ ‡§∏‡§Ç‡§ï‡•á‡§§ ‡§¶‡•á‡§§‡§æ ‡§π‡•à‡•§

üìä Live Streamlit Interface:
‚û°Ô∏è Simple, clean ‡§î‡§∞ secure UI ‡§ï‡•á ‡§∏‡§æ‡§• ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Ü‡§∏‡§æ‡§® Web App‡•§

 üß† Technologies Used

| Module                | Use                               |
| --------------------- | --------------------------------- |
| `scikit-learn`        | Text-based abuse classification   |
| `TF-IDF Vectorizer`   | Text vectorization                |
| `Logistic Regression` | Text classification               |
| `librosa`             | Audio feature extraction          |
| `NumPy`               | Numerical operations              |
| `DeepFace`            | Emotion analysis on facial images |
| `Streamlit`           | Web App framework                 |
| `Cloudflared`         | Public URL via ngrok replacement  |

üõ†Ô∏è Installation

```bash
!pip install streamlit numpy librosa deepface
!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared
!chmod +x cloudflared
```
üíº Model Training

```python
# Text Dataset = abusive_texts + non_abusive_texts
# Train Logistic Regression on TF-IDF vectors
# Save to models/text_model.pkl and models/text_vectorizer.pkl
```

‚úÖ Balanced dataset with üîÅ manually curated abusive & non-abusive phrases!

üì¶ App Structure

```
üìÅ models/
¬† ‚îú‚îÄ‚îÄ text_model.pkl
¬† ‚îî‚îÄ‚îÄ text_vectorizer.pkl
üìÑ app.py
```

üìå Note: `app.py` is automatically created and launched.

üîç Example Test Inputs

```python
test_samples = ['beautiful', 'hate', 'amazing', 'rape', 'friendly', 'stupid']
```

üëÅ‚Äçüó® Prediction Output:
`0 = Non-Abusive`, `1 = Abusive`

üåê Run the App (Google Colab Compatible)

```bash
!streamlit run app.py &>/content/logs.txt &
!./cloudflared tunnel --url http://localhost:8501 --no-autoupdate
```

üü¢ Wait a few seconds and your public link will appear!

üéØ Input Options in App

* Text üìú ‚Äì Enter plain text or statements.
* Audio üéô ‚Äì Upload `.wav` files.
* Image üñº ‚Äì Upload face image to detect emotions.

 üß† Emotion to Abuse Mapping (Image)

| Emotion          | Abuse Detected    |
| ---------------- | ----------------- |
| fear, angry, sad | Physical Abuse ‚ö†Ô∏è |
| others           | No Abuse ‚úÖ        |

 üôè Acknowledgements

üí° Inspired by real-world need for AI-powered women & child safety tools.
üì¢ Use it to create awareness, build support systems, or enhance existing safety applications.

 üë©‚Äçüíª Developer

> Developed by AI for Safety ‚ù§Ô∏è
> üí¨ Feel free to connect for collaboration or improvements!

üõë Disclaimer

> ‚ö†Ô∏è This is a prototype and should not be used as a substitute for professional help or legal action. For emergencies, contact official helplines.
